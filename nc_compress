#!/usr/bin/env python
import subprocess
import os
import sys
import netCDF4 as nc
import argparse
import re
from warnings import warn
from shutil import move
from collections import defaultdict
import math
import operator
import numpy as np
import numpy.ma as ma
import multiprocessing as mp


# A couple of hard-wired executable paths that might need changing

# Need to use system time command explicitly, otherwise get crippled bash version
timecmd='/usr/bin/time'
nccopy='nccopy'
nc2nc='nc2nc'
cdocmd='cdo'

def is_netCDF(ncfile):
    """ Test to see if ncfile is a valid netCDF file
    """
    try:
        tmp = nc.Dataset(ncfile)
        tmp.close()
        return True
    except RuntimeError:
        return False

def is_compressed(ncfile):
    """ Test if netcdfile is compressed
    """
    tmp = nc.Dataset(ncfile)
    compressed=False
    # netCDF3 files have no filters attribute, and no compression
    # should use data_model instead of file_format in future
    if not tmp.file_format.startswith("NETCDF3"):
        for varname in tmp.variables:
            hdf5_filters = tmp.variables[varname].filters()
            if hdf5_filters['complevel'] > 0:
                compressed = True
                break
        tmp.close()

    return compressed
        
def are_equal(infile,outfile,verbose):
    """ Run cdo diffn on the input and output netCDF files to ensure
        they are identical
    """
    cmd = [cdocmd,'diffn']
    cmd.append(infile)
    cmd.append(outfile)
    if verbose: print (' '.join(cmd))
    output = ''
    try:
        output = subprocess.check_output(cmd,stderr=subprocess.STDOUT)
    except:
        if verbose: print output
        return False
    return True 

def run_nc2nc(infile,outfile,level,shuffle,verbose=False,buffersize=None,chunking=None):
    # Check to see if the output file already exists ...
    if os.path.isfile(outfile):
        # Ok, we're going to be paranoid here, because this could be a left over
        # half compressed file from a previous run. We do not want to copy that
        # over our data
        if not are_equal(infile,outfile,verbose):
            sys.stdout.write("Output file %s exists, but is not the same as the input %s\n" % (outfile,infile))
            sys.stdout.write("Deleting output and recompressing\n")
            if os.path.isfile(outfile):
                os.unlink(outfile)
        else:
            if verbose: sys.stdout.write("Output file %s exists: skipping\n" % outfile)
            return {
                'infile' : infile,
                'outfile' : outfile,
                'times' : [-1,-1,-1,-1],
                'comp_size' : os.path.getsize(outfile),
                'orig_size' : os.path.getsize(infile),
                'dlevel' : level,
                'shuffle' : shuffle,
                'error' : False,
            }

    # The format string for the time command
    #   %e     (Not in tcsh.) Elapsed real time (in seconds).
    #   %S     Total number of CPU-seconds that the process spent in kernel mode.
    #   %U     Total number of CPU-seconds that the process spent in user mode.
    #   %M     Maximum resident set size of the process during its lifetime, in Kbytes.
    fmt = "%e %S %U %M"
    cmd = [timecmd,'-f',fmt,nc2nc,'-d',str(level)]
    if (not shuffle): cmd.append('-n')
    # if verbose: cmd.append('-v')
    if buffersize:
        cmd.append('-b')
        # All command line options have to be a string
        cmd.append(str(buffersize))
    if chunking:
        cmd.append('-c')
        cmd.append(chunking)
    cmd.append(infile)
    cmd.append(outfile)
    try:
        output = subprocess.check_output(cmd,stderr=subprocess.STDOUT)
        return {
            'infile' : infile,
            'outfile' : outfile,
            'times' : output.split(),
            'comp_size' : os.path.getsize(outfile),
            'orig_size' : os.path.getsize(infile),
            'dlevel' : level,
            'shuffle' : shuffle,
            'error' : False,
            }
    except:
        return {
            'infile' : infile,
            'outfile' : outfile,
            'error' : True,
            }

def run_nccopy(infile,outfile,level,shuffle,verbose=False,buffersize=None,chunking=None):
    # Check to see if the output file already exists ...
    if os.path.isfile(outfile):
        # Ok, we're going to be paranoid here, because this could be a left over
        # half compressed file from a previous run. We do not want to copy that
        # over our data
        if not are_equal(infile,outfile,verbose):
            sys.stdout.write("Output file %s exists, but is not the same as the input %s\n" % (outfile,infile))
            sys.stdout.write("Deleting output and recompressing\n")
            if os.path.isfile(outfile):
                os.unlink(outfile)
        else:
            if verbose: sys.stdout.write("Output file %s exists: skipping\n" % outfile)
            return {
                'infile' : infile,
                'outfile' : outfile,
                'times' : [-1,-1,-1,-1],
                'comp_size' : os.path.getsize(outfile),
                'orig_size' : os.path.getsize(infile),
                'dlevel' : level,
                'shuffle' : shuffle,
                'error' : False,
            }
    
    # The format string for the time command
    #   %e     (Not in tcsh.) Elapsed real time (in seconds).
    #   %S     Total number of CPU-seconds that the process spent in kernel mode.
    #   %U     Total number of CPU-seconds that the process spent in user mode.
    #   %M     Maximum resident set size of the process during its lifetime, in Kbytes.
    fmt = "%e %S %U %M"
    cmd = ['time','-f',fmt,nccopy,'-d',str(level)]
    if shuffle: cmd.append('-s')
    if chunking:
        cmd.append('-c')
        cmd.append(chunking)
    if buffersize:
        cmd.append('-m')
        cmd.append(str(buffersize*1000000))
    cmd.append(infile)
    cmd.append(outfile)
    if verbose: print (' '.join(cmd))
    try:
        output = subprocess.check_output(cmd,stderr=subprocess.STDOUT)
        return {
            'infile' : infile,
            'outfile' : outfile,
            'times' : output.split(),
            'comp_size' : os.path.getsize(outfile),
            'orig_size' : os.path.getsize(infile),
            'dlevel' : level,
            'shuffle' : shuffle,
            'error' : False,
        }
    except:
        return {
            'infile' : infile,
            'outfile' : outfile,
            'error' : True,
            }

result_list=[]

def log_result(result):
    # Not strictly required, but makes it explicit
    global result_list
    result_list.append(result)
    
def compress_files(path,files,tmpdir,overwrite,maxcompress,level,shuffle,force,clean,verbose,buffersize,nccopy,paranoid,numproc):

    total_size_new = 0
    total_size_old = 0
    total_files = 0
    skippedlist = []
    jobs = []

    global result_list
    result_list[:] = []

    pool = mp.Pool(processes=numproc,maxtasksperchild=50)

    copyprog = run_nccopy if nccopy else run_nc2nc

    # Create our temporary directory
    outdir = os.path.join(path,tmpdir)
    if not os.path.isdir(outdir):
        # Don't try and catch errors, let program stop if there is a problem
        os.mkdir(outdir)

    # Choose to clean all the files out of the tmp directory. We could
    # just delete them if we check to see if the output files exists
    # below, but this will only delete the files it is trying to compress.
    # If the program is called with a different list of files there may
    # be left over cruft and the directory won't be removed later. We need
    # a clean (ha ha) way for the user to sure all temporary files have
    # been removed.
    if clean:
        for file in os.listdir(outdir):
            file_path = os.path.join(outdir, file)
            try:
                if os.path.isfile(file_path):
                    os.unlink(file_path)
            except Exception, e:
                print e

    for file in files:

        infile = os.path.join(path,file)
        outfile = os.path.join(outdir,file)

        if verbose: sys.stdout.write( "Compressing %s, deflate level = %s, shuffle is on: %s\n" % (infile,level,shuffle) )

        # Make sure we're dealing with a netCDF file
        if not is_netCDF(infile):
            if verbose: print 'Not a netCDF file: ' + infile
            continue

        # Check to see if the input file is already compressed
        if is_compressed(infile):
            if force:
                if verbose: sys.stdout.write("Already compressed %s but forcing overwrite\n" % infile)
            else:
                if verbose: print 'Already compressed skipping ...'
                continue

        # Try compressing the data
        pool.apply_async(copyprog, args=(infile,outfile,level,shuffle,verbose,buffersize), callback=log_result)

    pool.close()
    pool.join()

    for result in result_list:

        # print result
        infile = result['infile']
        outfile = result['outfile']

        if result['error']:
            sys.stdout.write("Something went wrong with  %s \n" % infile)
            skippedlist.append(infile)
            # Go to next file .. so original will not be overwritten
            continue

        # Check compressed data is the same as original
        if paranoid:
            if not are_equal(infile,outfile,verbose):
                sys.stdout.write("%s is not the same as %s \n" % (infile,outfile))
                skippedlist.append(file)
                continue
                
        # Move compressed data back to original location
        if overwrite:

            # Serious. We're going to blow away the original file with
            # the compressed version -- do some sanity checks to make
            # sure we're not copying rubbish over our data
            if ( maxcompress != 0 and result['orig_size'] > maxcompress*result['comp_size'] ):
                # If the compressed version is less than 1/maxcompress we will
                # warn and not overwrite the original
                print("Compression ratio {0}, is greater than max compress ratio {1}: {2} not overwritten. Use -m option to change max compress ratio".format(result['orig_size']/result['comp_size'],maxcompress,infile))
                skippedlist.append(infile)
                continue
            
            move(outfile,infile)

        total_size_new += result['comp_size']
        total_size_old += result['orig_size']
        total_files = total_files + 1

        if verbose:
            print("{} d = {} Shuffle: {:d} {} s {} s {} s Mem: {} KB {} B {:0.4}".format(
                file, level, shuffle, 
                result['times'][0], result['times'][1], result['times'][2],
                result['times'][3], result['comp_size'], float(result['orig_size'])/float(result['comp_size'])))

    # Make a nice human readable number from the total amount of space we've saved
    total_space_saved = float(total_size_old-total_size_new)
    power = 0
    while (power < 4 and (int(total_space_saved) / 1000 > 0)):
        power = power + 1
        total_space_saved = total_space_saved / 1000. 
        
    units = ['B','KB','MB','GB','TB']

    if total_files > 0:
        print("Directory: {0}".format(path))
        print("    Number files compressed: {0}".format(total_files))
        print("    Total space saved: {0:.2f} {1}").format(total_space_saved,units[power])
        print("    Average compression ratio: {0:.2f}").format(float(total_size_old)/total_size_new)
    if len(skippedlist) > 0:
        print("    Following files not properly compressed or suspiciously high compression ratio:")
        print ("\n").join(skippedlist)

    try:
        os.rmdir(outdir)
    except OSError:
        print("Failed to remove temporary directory {}".format(outdir))

if __name__ == "__main__":

    def maxcompression_type(x):
        x = int(x)
        if x < 0:
            raise argparse.ArgumentTypeError("Minimum maxcompression is 0")
        return x

    
    parser = argparse.ArgumentParser(description="Run nc2nc (or nccopy) on a number of netCDF files")
    parser.add_argument("-d","--dlevel", help="Set deflate level. Valid values 0-9 (default=5)", type=int, default=5, choices=range(0,10), metavar='{1-9}')
    # parser.add_argument("-l","--limited", help="Change unlimited dimension to fixed size (default is to not squash unlimited)", action='store_true')
    parser.add_argument("-n","--noshuffle", help="Don't shuffle on deflation (default is to shuffle)", action='store_true')
    parser.add_argument("-b","--buffersize", help="Set size of copy buffer in MB (default=50)", type=int, default=50)
    parser.add_argument("-t","--tmpdir", help="Specify temporary directory to save compressed files", default='tmp.nc_compress')
    parser.add_argument("-v","--verbose", help="Verbose output", action='store_true')
    parser.add_argument("-r","--recursive", help="Recursively descend directories compressing all netCDF files (default False)", action='store_true')
    parser.add_argument("-o","--overwrite", help="Overwrite original files with compressed versions (default is to not overwrite)", action='store_true')
    parser.add_argument("-m","--maxcompress", help="Set a maximum compression as a paranoid check on success of nccopy (default is 10, set to zero for no check)", default=10,type=maxcompression_type)
    parser.add_argument("-p","--paranoid", help="Paranoid check : run nco ndiff on the resulting file ensure no data has been altered", action='store_true')
    parser.add_argument("-f","--force", help="Force compression, even if input file is already compressed (default False)", action='store_true')
    parser.add_argument("-c","--clean", help="Clean tmpdir by removing existing compressed files before starting (default False)", action='store_true')
    parser.add_argument("-pa","--parallel", help="Compress files in parallel", action='store_true')
    parser.add_argument("-np","--numproc", help="Specify the number of processes to use in parallel operation", type=int, default=1)
    parser.add_argument("--nccopy", help="Use nccopy instead of nc2nc (default False)", action='store_true')
    parser.add_argument("inputs", help="netCDF files or directories (-r must be specified to recursively descend directories)", nargs='+')
    args = parser.parse_args()
    
    verbose=args.verbose

    # We won't make users specify parallel if they've specified a number of processors
    if args.numproc: args.parallel = True

    if args.parallel:
        if args.numproc is not None:
            numproc = args.numproc
        else:
            numproc = mp.cpu_count()

    filedict = defaultdict(list)

    # Loop over all the inputs from the command line. These can be either file globs
    # or directory names. In either case we'll group them by directory
    for ncinput in args.inputs:
        if not os.path.exists(ncinput):
            print ("Input does not exist: {} .. skipping".format(ncinput))
            continue
        if os.path.isdir(ncinput):
            # os.walk will return the entire directory structure
            for root, dirs, files in os.walk(ncinput):
                # Ignore emtpy directories, and our own temp directory, in case we
                # re-run on same tree
                if len(files) == 0: continue
                if root.endswith(args.tmpdir): continue
                # Only descend into subdirs if we've set the recursive flag
                if (root != ncinput and not args.recursive):
                    print("Skipping subdirectory {0} :: --recursive option not specified".format(root))
                    continue
                else:
                    # Group by directory
                    filedict[root].extend(files)
        else:
            (root,file) = os.path.split(ncinput)
            if (root == ''): root = "./"
            filedict[root].append(file)

    if len(filedict) == 0:
        print "No files found to process"
    else:
        # Compress files directory by directory. We only create a temporary directory once,
        # and can then clean up after ourselves. Also makes it easier to run some checks to
        # ensure compression is ok, as all the files are named the same, just in a separate
        # temporary sub directory.
        for directory in filedict:
            if len(filedict[directory]) == 0: continue
            compress_files(directory,filedict[directory],args.tmpdir,args.overwrite,args.maxcompress,args.dlevel,not args.noshuffle,args.force,args.clean,verbose,args.buffersize,args.nccopy,args.paranoid,numproc)

                
